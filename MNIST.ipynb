{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt, matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name='X')\n",
    "y = tf.placeholder(tf.float32, [None, 10],name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]), name='W')\n",
    "b = tf.Variable(tf.zeros([10]), name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"wx_b\") as scope:\n",
    "    y_hat = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = tf.summary.histogram(\"weights\", W)\n",
    "b_h = tf.summary.histogram(\"biases\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-14-1879e2a7f7b6>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-1879e2a7f7b6>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_hat)\u001b[0m\n\u001b[1;37m                                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('cross-entropy') as scope:\n",
    "    loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cross-entropy') as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_hat, labels = y))\n",
    "    tf.summary.scalar('cross-entropy', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Train') as scope:\n",
    "    optimizer =tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 2.0928163631831116\n",
      "Done\n",
      "0.7205\n",
      "Epoch 1: Loss 1.8313929582225013\n",
      "Done\n",
      "0.7984\n",
      "Epoch 2: Loss 1.7603564106266212\n",
      "Done\n",
      "0.81\n",
      "Epoch 3: Loss 1.730005382103748\n",
      "Done\n",
      "0.8168\n",
      "Epoch 4: Loss 1.71278227284568\n",
      "Done\n",
      "0.8209\n",
      "Epoch 5: Loss 1.7004646858042138\n",
      "Done\n",
      "0.8243\n",
      "Epoch 6: Loss 1.692176127600309\n",
      "Done\n",
      "0.8266\n",
      "Epoch 7: Loss 1.6853173855991386\n",
      "Done\n",
      "0.8292\n",
      "Epoch 8: Loss 1.6747660017957233\n",
      "Done\n",
      "0.8544\n",
      "Epoch 9: Loss 1.657165520751851\n",
      "Done\n",
      "0.8709\n",
      "Epoch 10: Loss 1.644724792734708\n",
      "Done\n",
      "0.881\n",
      "Epoch 11: Loss 1.634051543800323\n",
      "Done\n",
      "0.8883\n",
      "Epoch 12: Loss 1.6265523865975102\n",
      "Done\n",
      "0.8929\n",
      "Epoch 13: Loss 1.621329216934888\n",
      "Done\n",
      "0.8957\n",
      "Epoch 14: Loss 1.6166137933592302\n",
      "Done\n",
      "0.8971\n",
      "Epoch 15: Loss 1.6131872729039443\n",
      "Done\n",
      "0.8982\n",
      "Epoch 16: Loss 1.6103462474171302\n",
      "Done\n",
      "0.8996\n",
      "Epoch 17: Loss 1.6073079195233524\n",
      "Done\n",
      "0.9\n",
      "Epoch 18: Loss 1.6051305118900516\n",
      "Done\n",
      "0.9009\n",
      "Epoch 19: Loss 1.6026128049640633\n",
      "Done\n",
      "0.9017\n",
      "Epoch 20: Loss 1.600784322782501\n",
      "Done\n",
      "0.903\n",
      "Epoch 21: Loss 1.5988570614799216\n",
      "Done\n",
      "0.9039\n",
      "Epoch 22: Loss 1.5974904943400683\n",
      "Done\n",
      "0.9047\n",
      "Epoch 23: Loss 1.5955216826326772\n",
      "Done\n",
      "0.9047\n",
      "Epoch 24: Loss 1.594542931899758\n",
      "Done\n",
      "0.9055\n",
      "Epoch 25: Loss 1.5933438898243089\n",
      "Done\n",
      "0.9059\n",
      "Epoch 26: Loss 1.59132374568091\n",
      "Done\n",
      "0.9065\n",
      "Epoch 27: Loss 1.5906289766636938\n",
      "Done\n",
      "0.907\n",
      "Epoch 28: Loss 1.589547915286596\n",
      "Done\n",
      "0.9072\n",
      "Epoch 29: Loss 1.5883098885815967\n",
      "Done\n",
      "0.9079\n",
      "Epoch 30: Loss 1.5873814402137285\n",
      "Done\n",
      "0.9088\n",
      "Epoch 31: Loss 1.5864366426873124\n",
      "Done\n",
      "0.9092\n",
      "Epoch 32: Loss 1.5855526277433314\n",
      "Done\n",
      "0.909\n",
      "Epoch 33: Loss 1.5845929282924487\n",
      "Done\n",
      "0.9093\n",
      "Epoch 34: Loss 1.5839419656360525\n",
      "Done\n",
      "0.9094\n",
      "Epoch 35: Loss 1.5831116449014133\n",
      "Done\n",
      "0.9092\n",
      "Epoch 36: Loss 1.5825420566709827\n",
      "Done\n",
      "0.9094\n",
      "Epoch 37: Loss 1.5816391339401983\n",
      "Done\n",
      "0.9093\n",
      "Epoch 38: Loss 1.5806437442133396\n",
      "Done\n",
      "0.9096\n",
      "Epoch 39: Loss 1.5803673637898614\n",
      "Done\n",
      "0.9102\n",
      "Epoch 40: Loss 1.5796732671463447\n",
      "Done\n",
      "0.9104\n",
      "Epoch 41: Loss 1.5791876905872047\n",
      "Done\n",
      "0.9107\n",
      "Epoch 42: Loss 1.5784169797708603\n",
      "Done\n",
      "0.911\n",
      "Epoch 43: Loss 1.5776644487181142\n",
      "Done\n",
      "0.9114\n",
      "Epoch 44: Loss 1.5774221269021351\n",
      "Done\n",
      "0.9114\n",
      "Epoch 45: Loss 1.5766664560238057\n",
      "Done\n",
      "0.912\n",
      "Epoch 46: Loss 1.576600925747535\n",
      "Done\n",
      "0.912\n",
      "Epoch 47: Loss 1.5753855810731614\n",
      "Done\n",
      "0.9123\n",
      "Epoch 48: Loss 1.5754617091801724\n",
      "Done\n",
      "0.9124\n",
      "Epoch 49: Loss 1.5747398567560527\n",
      "Done\n",
      "0.9131\n",
      "Epoch 50: Loss 1.574561781253193\n",
      "Done\n",
      "0.9132\n",
      "Epoch 51: Loss 1.573747686512673\n",
      "Done\n",
      "0.9136\n",
      "Epoch 52: Loss 1.5736039636160082\n",
      "Done\n",
      "0.9139\n",
      "Epoch 53: Loss 1.5728361487249833\n",
      "Done\n",
      "0.9139\n",
      "Epoch 54: Loss 1.5727681172046728\n",
      "Done\n",
      "0.9146\n",
      "Epoch 55: Loss 1.572557485422794\n",
      "Done\n",
      "0.915\n",
      "Epoch 56: Loss 1.5717926069105324\n",
      "Done\n",
      "0.9146\n",
      "Epoch 57: Loss 1.5715849226095624\n",
      "Done\n",
      "0.9152\n",
      "Epoch 58: Loss 1.571036276079029\n",
      "Done\n",
      "0.9154\n",
      "Epoch 59: Loss 1.5707349295666109\n",
      "Done\n",
      "0.9155\n",
      "Epoch 60: Loss 1.5704750531766134\n",
      "Done\n",
      "0.9155\n",
      "Epoch 61: Loss 1.5700457109283652\n",
      "Done\n",
      "0.9158\n",
      "Epoch 62: Loss 1.5699648834218525\n",
      "Done\n",
      "0.9158\n",
      "Epoch 63: Loss 1.5693479906138774\n",
      "Done\n",
      "0.9159\n",
      "Epoch 64: Loss 1.5692542055849423\n",
      "Done\n",
      "0.9163\n",
      "Epoch 65: Loss 1.5688862044958352\n",
      "Done\n",
      "0.9166\n",
      "Epoch 66: Loss 1.5681490082124614\n",
      "Done\n",
      "0.9171\n",
      "Epoch 67: Loss 1.568176113754268\n",
      "Done\n",
      "0.9166\n",
      "Epoch 68: Loss 1.5679810664706513\n",
      "Done\n",
      "0.9172\n",
      "Epoch 69: Loss 1.5676576251617271\n",
      "Done\n",
      "0.9174\n",
      "Epoch 70: Loss 1.5671793567287768\n",
      "Done\n",
      "0.9176\n",
      "Epoch 71: Loss 1.5671150937624387\n",
      "Done\n",
      "0.9174\n",
      "Epoch 72: Loss 1.567214251536013\n",
      "Done\n",
      "0.918\n",
      "Epoch 73: Loss 1.5662630876923052\n",
      "Done\n",
      "0.918\n",
      "Epoch 74: Loss 1.5661838450170924\n",
      "Done\n",
      "0.918\n",
      "Epoch 75: Loss 1.5663991130550194\n",
      "Done\n",
      "0.9181\n",
      "Epoch 76: Loss 1.5657671030797282\n",
      "Done\n",
      "0.9182\n",
      "Epoch 77: Loss 1.565406263707264\n",
      "Done\n",
      "0.9183\n",
      "Epoch 78: Loss 1.5653222220601803\n",
      "Done\n",
      "0.9184\n",
      "Epoch 79: Loss 1.5652401985750208\n",
      "Done\n",
      "0.9183\n",
      "Epoch 80: Loss 1.5649455128643093\n",
      "Done\n",
      "0.9189\n",
      "Epoch 81: Loss 1.5646574181505633\n",
      "Done\n",
      "0.9189\n",
      "Epoch 82: Loss 1.564130546397186\n",
      "Done\n",
      "0.919\n",
      "Epoch 83: Loss 1.5641961722906865\n",
      "Done\n",
      "0.9192\n",
      "Epoch 84: Loss 1.5638933988056027\n",
      "Done\n",
      "0.9194\n",
      "Epoch 85: Loss 1.5638530467386433\n",
      "Done\n",
      "0.9195\n",
      "Epoch 86: Loss 1.5636990536633137\n",
      "Done\n",
      "0.9192\n",
      "Epoch 87: Loss 1.563085124295582\n",
      "Done\n",
      "0.9194\n",
      "Epoch 88: Loss 1.5630469027303844\n",
      "Done\n",
      "0.9194\n",
      "Epoch 89: Loss 1.5630240901801584\n",
      "Done\n",
      "0.9196\n",
      "Epoch 90: Loss 1.5628116282789477\n",
      "Done\n",
      "0.9198\n",
      "Epoch 91: Loss 1.5624694349602022\n",
      "Done\n",
      "0.9195\n",
      "Epoch 92: Loss 1.5623355584177898\n",
      "Done\n",
      "0.9197\n",
      "Epoch 93: Loss 1.562237154410521\n",
      "Done\n",
      "0.9194\n",
      "Epoch 94: Loss 1.5618759249364123\n",
      "Done\n",
      "0.9198\n",
      "Epoch 95: Loss 1.5616461990598471\n",
      "Done\n",
      "0.9194\n",
      "Epoch 96: Loss 1.5620148204813458\n",
      "Done\n",
      "0.9198\n",
      "Epoch 97: Loss 1.5609616452656747\n",
      "Done\n",
      "0.92\n",
      "Epoch 98: Loss 1.561494599745909\n",
      "Done\n",
      "0.9202\n",
      "Epoch 99: Loss 1.5609083605867327\n",
      "Done\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "max_epochs=100\n",
    "batch_size=32\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize all variables\n",
    "    summary_writer = tf.summary.FileWriter('graphs', sess.graph)\n",
    "    #Create an event file\n",
    "    # Training\n",
    "    for epoch in range(max_epochs):\n",
    "        loss_avg = 0\n",
    "        num_of_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(num_of_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # getthe next batch of data\n",
    "            _, l, summary_str = sess.run([optimizer,loss,merged_summary_op], feed_dict={x: batch_xs, y: batch_ys})\n",
    "    # Run the optimizer\n",
    "            loss_avg += l\n",
    "            summary_writer.add_summary(summary_str,epoch*num_of_batch + i)\n",
    "            # Add all summaries per batch\n",
    "         # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(y_hat, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        loss_avg = loss_avg/num_of_batch\n",
    "        print('Epoch {0}: Loss {1}'.format(epoch, loss_avg))\n",
    "        print('Done')\n",
    "        print(sess.run(accuracy, feed_dict={x: mnist.test.images,y:mnist.test.labels}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
